{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c6c5c8",
   "metadata": {},
   "source": [
    "# Lab 4: Bag of Visual Words (BoVW) for Image Classification\n",
    "\n",
    "## Objective\n",
    "Implement the Bag of Visual Words technique for image classification using:\n",
    "- UC Merced Land Use Dataset\n",
    "- SIFT features for local descriptor extraction\n",
    "- K-means clustering for visual vocabulary creation\n",
    "- SVM classifier for classification\n",
    "\n",
    "## Pipeline Steps:\n",
    "1. Data Loading and Preprocessing\n",
    "2. Dataset Splitting (80% train, 20% test)\n",
    "3. SIFT Feature Extraction\n",
    "4. Building Visual Vocabulary (K-means clustering)\n",
    "5. Quantization (assign descriptors to visual words)\n",
    "6. Histogram Representation\n",
    "7. Normalization (L1 or L2)\n",
    "8. Classifier Training (SVM)\n",
    "9. Prediction Generation\n",
    "10. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db423f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f857afc",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9625a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_images_and_labels(dataset_root):\n",
    "    \"\"\"\n",
    "    Load all images and their corresponding labels from the dataset directory.\n",
    "    \n",
    "    Args:\n",
    "        dataset_root: Path to the dataset root directory\n",
    "        \n",
    "    Returns:\n",
    "        images: List of image file paths\n",
    "        labels: List of corresponding class labels\n",
    "        classes: List of unique class names\n",
    "    \"\"\"\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Get all class directories (sorted for consistency)\n",
    "    classes = sorted([d for d in os.listdir(dataset_root) \n",
    "                     if os.path.isdir(os.path.join(dataset_root, d))])\n",
    "    \n",
    "    # Iterate through each class directory\n",
    "    for cls in classes:\n",
    "        cls_dir = os.path.join(dataset_root, cls)\n",
    "        for fname in os.listdir(cls_dir):\n",
    "            # Load .tif files (UC Merced dataset format)\n",
    "            if fname.lower().endswith(\".tif\"):\n",
    "                images.append(os.path.join(cls_dir, fname))\n",
    "                labels.append(cls)\n",
    "    \n",
    "    return images, labels, classes\n",
    "\n",
    "# Load dataset\n",
    "DATASET_PATH = \"./Images\"  # Update this path if needed\n",
    "images, labels, class_names = list_images_and_labels(DATASET_PATH)\n",
    "\n",
    "if not images:\n",
    "    raise RuntimeError(\"No images found. Check dataset path and structure.\")\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Total classes: {len(class_names)}\")\n",
    "print(f\"Total images: {len(images)}\")\n",
    "print(f\"Classes: {class_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969d79e",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Splitting (80% Train, 20% Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90af2693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training (80%) and testing (20%) sets\n",
    "# Use stratify to maintain class balance\n",
    "X_train_paths, X_test_paths, y_train, y_test = train_test_split(\n",
    "    images, \n",
    "    labels, \n",
    "    test_size=0.2,  # 20% for testing\n",
    "    stratify=labels,  # Maintain class distribution\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train_paths)} images\")\n",
    "print(f\"Testing set: {len(X_test_paths)} images\")\n",
    "print(f\"Split ratio: {len(X_train_paths)/len(images)*100:.1f}% train, {len(X_test_paths)/len(images)*100:.1f}% test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6847630",
   "metadata": {},
   "source": [
    "## Step 3: SIFT Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96333fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_preprocess(path, size=(256, 256)):\n",
    "    \"\"\"\n",
    "    Read and preprocess an image.\n",
    "    \n",
    "    Args:\n",
    "        path: Image file path\n",
    "        size: Target size for resizing\n",
    "        \n",
    "    Returns:\n",
    "        Preprocessed grayscale image\n",
    "    \"\"\"\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Unable to read image: {path}\")\n",
    "    # Resize for consistency\n",
    "    img = cv2.resize(img, size, interpolation=cv2.INTER_AREA)\n",
    "    return img\n",
    "\n",
    "# Initialize SIFT detector\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "print(\"Extracting SIFT descriptors from training images...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "train_descriptors_list = []  # Store descriptors for each image\n",
    "all_train_descriptors = []   # Store all descriptors for K-means\n",
    "\n",
    "# Extract SIFT features from training images\n",
    "for path in tqdm(X_train_paths, desc=\"SIFT extraction (train)\"):\n",
    "    img = read_and_preprocess(path)\n",
    "    \n",
    "    # Detect keypoints and compute descriptors\n",
    "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "    \n",
    "    # Handle images with no keypoints\n",
    "    if descriptors is None:\n",
    "        descriptors = np.zeros((1, 128), dtype=np.float32)\n",
    "    \n",
    "    train_descriptors_list.append(descriptors)\n",
    "    all_train_descriptors.append(descriptors)\n",
    "\n",
    "# Stack all descriptors for K-means clustering\n",
    "all_train_descriptors = np.vstack(all_train_descriptors).astype(np.float32)\n",
    "\n",
    "print(f\"\\nSIFT extraction completed!\")\n",
    "print(f\"Total descriptors extracted: {all_train_descriptors.shape[0]}\")\n",
    "print(f\"Descriptor dimension: {all_train_descriptors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c4891f",
   "metadata": {},
   "source": [
    "## Step 4: Building Visual Vocabulary with K-means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ea1fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build visual vocabulary using K-means clustering\n",
    "# Each cluster center represents a \"visual word\"\n",
    "\n",
    "K = 100  # Number of visual words (cluster centers)\n",
    "\n",
    "print(f\"\\nBuilding visual vocabulary with K={K} visual words...\")\n",
    "print(\"Running K-means clustering...\")\n",
    "\n",
    "# Use MiniBatchKMeans for faster processing with large datasets\n",
    "kmeans = MiniBatchKMeans(\n",
    "    n_clusters=K,\n",
    "    batch_size=1000,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit K-means on all training descriptors\n",
    "kmeans.fit(all_train_descriptors)\n",
    "\n",
    "# Visual vocabulary = cluster centers\n",
    "visual_vocabulary = kmeans.cluster_centers_\n",
    "\n",
    "print(f\"\\nVisual vocabulary created!\")\n",
    "print(f\"Vocabulary shape: {visual_vocabulary.shape}\")\n",
    "print(f\"Each visual word is a {visual_vocabulary.shape[1]}-dimensional vector\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14080b29",
   "metadata": {},
   "source": [
    "## Step 5: Quantization and Histogram Representation\n",
    "\n",
    "Quantization: Assign each local descriptor to its nearest visual word.  \n",
    "Histogram: Count occurrences of each visual word in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bb6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_histogram(descriptors, kmeans_model, k):\n",
    "    \"\"\"\n",
    "    Create histogram of visual words for an image.\n",
    "    \n",
    "    Args:\n",
    "        descriptors: SIFT descriptors of the image\n",
    "        kmeans_model: Trained K-means model\n",
    "        k: Number of clusters (visual words)\n",
    "        \n",
    "    Returns:\n",
    "        Histogram of visual word occurrences\n",
    "    \"\"\"\n",
    "    # Quantization: Assign each descriptor to nearest cluster center\n",
    "    visual_words = kmeans_model.predict(descriptors)\n",
    "    \n",
    "    # Create histogram: Count occurrences of each visual word\n",
    "    histogram = np.zeros(k)\n",
    "    for word in visual_words:\n",
    "        histogram[word] += 1\n",
    "    \n",
    "    return histogram\n",
    "\n",
    "print(\"Creating histograms for training images...\")\n",
    "\n",
    "# Create histograms for all training images\n",
    "X_train_histograms = []\n",
    "for descriptors in tqdm(train_descriptors_list, desc=\"Creating histograms (train)\"):\n",
    "    hist = create_histogram(descriptors, kmeans, K)\n",
    "    X_train_histograms.append(hist)\n",
    "\n",
    "X_train_histograms = np.array(X_train_histograms)\n",
    "\n",
    "print(f\"\\nHistograms created!\")\n",
    "print(f\"Training histogram matrix shape: {X_train_histograms.shape}\")\n",
    "print(f\"Each image represented as {K}-dimensional histogram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f26e56",
   "metadata": {},
   "source": [
    "## Step 6: Normalization\n",
    "\n",
    "Normalize histograms to account for variations in the number of features per image.\n",
    "\n",
    "- **L1 Normalization**: Sum of histogram values = 1 (frequency distribution)\n",
    "- **L2 Normalization**: Sum of squares = 1 (scale invariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11731d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose normalization method: 'l1' or 'l2'\n",
    "NORMALIZATION = 'l2'  # Change to 'l1' if desired\n",
    "\n",
    "print(f\"Applying {NORMALIZATION.upper()} normalization...\")\n",
    "\n",
    "# Normalize histograms\n",
    "if NORMALIZATION == 'l1':\n",
    "    # L1: Sum of values = 1\n",
    "    X_train_normalized = normalize(X_train_histograms, norm='l1', axis=1)\n",
    "    print(\"L1 normalization: Each histogram sums to 1\")\n",
    "elif NORMALIZATION == 'l2':\n",
    "    # L2: Sum of squares = 1\n",
    "    X_train_normalized = normalize(X_train_histograms, norm='l2', axis=1)\n",
    "    print(\"L2 normalization: Euclidean norm = 1\")\n",
    "\n",
    "print(f\"Normalized training data shape: {X_train_normalized.shape}\")\n",
    "\n",
    "# Verify normalization\n",
    "if NORMALIZATION == 'l1':\n",
    "    print(f\"Sample L1 norm (should be ~1.0): {np.sum(X_train_normalized[0]):.6f}\")\n",
    "elif NORMALIZATION == 'l2':\n",
    "    print(f\"Sample L2 norm (should be ~1.0): {np.linalg.norm(X_train_normalized[0]):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e5bb90",
   "metadata": {},
   "source": [
    "## Step 7: Classifier Training (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbfef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining SVM classifier...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "# Train Support Vector Machine classifier\n",
    "svm_classifier = SVC(\n",
    "    kernel='rbf',      # Radial Basis Function kernel\n",
    "    C=10,              # Regularization parameter\n",
    "    gamma='scale',     # Kernel coefficient\n",
    "    random_state=42,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Train on normalized histograms and corresponding labels\n",
    "svm_classifier.fit(X_train_normalized, y_train)\n",
    "\n",
    "print(\"\\nSVM training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea5e09d",
   "metadata": {},
   "source": [
    "## Step 8: Generate Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456f2ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Processing test images...\")\n",
    "\n",
    "# Extract SIFT features from test images\n",
    "test_descriptors_list = []\n",
    "\n",
    "for path in tqdm(X_test_paths, desc=\"SIFT extraction (test)\"):\n",
    "    img = read_and_preprocess(path)\n",
    "    keypoints, descriptors = sift.detectAndCompute(img, None)\n",
    "    \n",
    "    if descriptors is None:\n",
    "        descriptors = np.zeros((1, 128), dtype=np.float32)\n",
    "    \n",
    "    test_descriptors_list.append(descriptors)\n",
    "\n",
    "print(\"\\nCreating histograms for test images...\")\n",
    "\n",
    "# Create and normalize histograms for test images\n",
    "X_test_histograms = []\n",
    "for descriptors in tqdm(test_descriptors_list, desc=\"Creating histograms (test)\"):\n",
    "    hist = create_histogram(descriptors, kmeans, K)\n",
    "    X_test_histograms.append(hist)\n",
    "\n",
    "X_test_histograms = np.array(X_test_histograms)\n",
    "\n",
    "# Apply same normalization as training data\n",
    "X_test_normalized = normalize(X_test_histograms, norm=NORMALIZATION, axis=1)\n",
    "\n",
    "print(f\"\\nTest data prepared!\")\n",
    "print(f\"Test histogram matrix shape: {X_test_normalized.shape}\")\n",
    "\n",
    "# Generate predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred = svm_classifier.predict(X_test_normalized)\n",
    "print(\"Predictions completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05bf63a",
   "metadata": {},
   "source": [
    "## Step 9: Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a44e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nClassification Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(f\"Correctly classified: {int(accuracy * len(y_test))}/{len(y_test)} images\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"\\nDetailed Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0000b790",
   "metadata": {},
   "source": [
    "## Step 10: Confusion Matrix Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97c40dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=class_names)\n",
    "\n",
    "# Create figure\n",
    "plt.figure(figsize=(16, 14))\n",
    "\n",
    "# Plot confusion matrix\n",
    "sns.heatmap(\n",
    "    cm, \n",
    "    annot=True,           # Show numbers\n",
    "    fmt='d',              # Integer format\n",
    "    cmap='Blues',         # Color scheme\n",
    "    xticklabels=class_names,\n",
    "    yticklabels=class_names,\n",
    "    cbar_kws={'label': 'Number of Predictions'}\n",
    ")\n",
    "\n",
    "plt.title(f'Confusion Matrix - BoVW Image Classification\\nAccuracy: {accuracy*100:.2f}%', \n",
    "          fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Predicted Label', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f6864e",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "\n",
    "### Pipeline Parameters:\n",
    "- **Dataset**: UC Merced Land Use Dataset\n",
    "- **Train/Test Split**: 80% / 20%\n",
    "- **Feature Extractor**: SIFT\n",
    "- **Visual Vocabulary Size (K)**: 100 visual words\n",
    "- **Normalization**: L2\n",
    "- **Classifier**: SVM with RBF kernel\n",
    "\n",
    "### Key Concepts:\n",
    "1. **SIFT Features**: Local image descriptors invariant to scale and rotation\n",
    "2. **Visual Words**: Cluster centers from K-means representing common local patterns\n",
    "3. **Quantization**: Mapping local descriptors to nearest visual word\n",
    "4. **Histogram**: Frequency distribution of visual words in an image\n",
    "5. **BoVW**: Each image represented as a histogram of visual word occurrences\n",
    "\n",
    "### Next Steps for Improvement:\n",
    "- Experiment with different K values (50, 150, 200)\n",
    "- Try different normalization methods (L1 vs L2)\n",
    "- Tune SVM hyperparameters (C, gamma)\n",
    "- Test other classifiers (Random Forest, Neural Networks)\n",
    "- Use dense SIFT or other feature extractors (ORB, SURF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21173ecb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
